Integrate Spark with R

   You integrate Spark with R when you want to run R programs as Spark
   jobs.
    1. On each node that will submit Spark jobs, install R 3.2.2 or
       greater:
          + On Ubuntu:
apt-get install r-base-dev
          + On CentOS/RedHat:
yum install R
       For more information about installing R, see the [1]R
       documentation.
    2. To verify the integration, run the following commands as the mapr
       user or as a user that mapr impersonates:
         a. Start Spark R:
               o On Spark 2.0.1 and 2.1.0:
/opt/mapr/spark/spark-<version>/bin/sparkR --master <master> [--deploy-mode <dep
loy-mode>]
               o On Spark 1.6.1:
/opt/mapr/spark/spark-<version>/bin/sparkR --master <master-url>
         b. Run the following command to create a DataFrame using sample
            data:
            On Spark 1.6.1:
people <- read.df(sqlContext, "file:///opt/mapr/spark/spark-<version>/examples/s
rc/main/resources/people.json", "json")
            On Spark 2.0.1 and 2.1.0:
people <- read.df(spark, "file:///opt/mapr/spark/spark-<version>/examples/src/ma
in/resources/people.json", "json")
         c. Run the following command to display the data from the
            DataFrame that you just created:
head(people)

References

   1. https://cran.r-project.org/doc/FAQ/R-FAQ.html
