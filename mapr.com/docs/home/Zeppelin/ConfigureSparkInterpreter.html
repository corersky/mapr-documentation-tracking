Configuring the Spark Interpreter

   The Spark interpreter is available starting in the 1.1 release of the
   MapR Data Science Refinery. It provides support for Spark Python,
   SparkR, Basic Spark, and Spark SQL jobs. To use the Spark interpreter
   for these variations of Spark, you must take certain actions, including
   configuring Zeppelin and installing software on your MapR cluster.

   You must also issue your docker run command with the parameters the
   Spark interpreter requires. See the following material for details
   about these parameters:
     * [1]Bridge Networking
     * [2]Pig, Livy, and Spark Interpreters

Spark Python

   The Zeppelin container includes Python 2.x. You must also install it on
   your MapR cluster to run Python code with the Spark interpreter. If the
   version installed in your MapR cluster nodes is different from the
   version included in the container, some functionality may not work.
   To use Python in the Spark interpreter, specify the following in your
   notebook:
%spark.pyspark

   By default, this invokes Python 2. Python 3 is not included in the
   Zeppelin container. To install custom Python packages, see
   [3]Installing Custom Packages for PySpark. This also describes how to
   use Python 3 with custom packages.
   Note: The Spark interpreter supports running only one version of Python
   in your Zeppelin container.

SparkR

   The Zeppelin container includes R. Some Apache SparkR jobs require you
   to install R on your MapR cluster nodes to run these jobs in the Spark
   interpreter. If the version installed in your MapR cluster nodes is
   different from the version included in the container, some
   functionality may not work.
   To use R in the Spark interpreter, specify the following in your
   notebook:
%spark.r

Spark Jobs

   By default, the Spark interpreter is configured to submit Apache Spark
   jobs in YARN client mode. The interpreter does not support YARN cluster
   mode. Make sure you follow the steps described at [4]Installing Spark
   on YARN to install Spark on your MapR cluster.

   To run Spark jobs in parallel, you must modify the Spark interpreter to
   instantiate Per Note:
   [SparkInterpreter.png]

   You can set scoped to either of the two options.

Hive Tables

   To access Apache Hive tables using the Spark interpreter, you must make
   the hive-site.xml configuration file from your Hive cluster available
   to Spark running in your Zeppelin container. Follow the same steps that
   describe [5]how to access Hive tables with the Livy interpreter.

References

   1. file://localhost/root/docsync/tmp/mapr.com/docs/home/Zeppelin/ZeppelinDockerRunParameters.html#concept_rhn_gb2_rbb__section_h1n_pc2_rbb
   2. file://localhost/root/docsync/tmp/mapr.com/docs/home/Zeppelin/ZeppelinDockerRunParameters.html#concept_rhn_gb2_rbb__section_kfb_k32_3db
   3. file://localhost/root/docsync/tmp/mapr.com/docs/home/Zeppelin/InstallCustomPySpark.html#task_nfn_qnc_ybb
   4. file://localhost/root/docsync/tmp/mapr.com/docs/home/AdvancedInstallation/InstallSparkonYARN.html#InstallSparkonYARN
   5. file://localhost/root/docsync/tmp/mapr.com/docs/home/Zeppelin/ConfigureLivyInterpreter.html#task_t1d_4yj_qbb__section_p4h_sdk_qbb
